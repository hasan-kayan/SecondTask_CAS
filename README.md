# SecondTask_CAS

  ## Udemy Course Cupon Scraper Task 


### Coding All Stars Second Task 


![web-scraping-with-python](https://user-images.githubusercontent.com/80827760/223840296-d74ccc09-77ce-46f5-b89f-d5c499e0e5a3.png)



# Udemy Coupon Scraper

This is a Python script that scrapes Udemy coupons from three different websites: real.discount, discudemy.com, and couponscorpion.com. It uses Selenium and BeautifulSoup libraries to navigate the webpages, extract data and write to a CSV file.

Before we start lets make a preparation. 

### NOTE : The code was previously prepared on the Linux operating system, but it was organized according to Windows. If you are going to run it on a Linux or Mac device, you need to make path arrangements.

## Prerequests 

- Python 3.11.2 or later version should be installed on your system.
- Chrome driver only supports for 111 version, so be check your Chrome browser version. 
- Before you run the code you need to install all libraries, to do that I created a 'requirements.txt' file you cann call it at the terminal to install all needed libraries. How call 'requirements.txt' :
    . Open your terminal
    .Run this command : pip install -r requirements.txt

Now lets make these steps together...

- Open Visal Studio Code --> File --> Open Folder --> SecondTask_CAS 
- Open a new terminal 
- Run " pip install -r requirements.txt " command at terminal:

![image](https://user-images.githubusercontent.com/80827760/223865374-9f7d883b-e2b8-4a76-8d0e-a7a481b230a3.png)
For me, it says "Requirement already satisfied:" but in your computer packages will be downloading.รง 
- Now lets run the script by the command: " python scrapy.py " 
- It will ask for page number that you want enter those values as you wish. 
- ![image](https://user-images.githubusercontent.com/80827760/223866240-3aab3958-37dc-4529-8f77-ec92793ea130.png)
It will start running image above and tell you what it is doing step by step. 
#### In this process, some pages may be opened and closed, you should not interfere with these pages manually.

![image](https://user-images.githubusercontent.com/80827760/223866017-082dafc4-adc8-4b61-a567-73945e02b944.png)

This takes a while, do not close the program, it says if there is an error. 

- 

























